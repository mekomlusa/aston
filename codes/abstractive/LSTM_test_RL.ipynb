{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for auto text summarization\n",
    "\n",
    "Playground. Will fulfilled this later.\n",
    "\n",
    "Most of the setup codes are adapted from [this repo](https://github.com/chen0040/keras-text-summarization/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load require libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 20  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 2000  # Number of samples to train on.\n",
    "\n",
    "MAX_INPUT_SEQ_LENGTH = 3000\n",
    "MAX_TARGET_SEQ_LENGTH = 300\n",
    "MAX_INPUT_VOCAB_SIZE = 15000\n",
    "MAX_TARGET_VOCAB_SIZE = 6000\n",
    "DEFAULT_EPOCH_SIZE = epochs = 20\n",
    "DEFAULT_BATCH_SIZE = batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "text_path = \"/home/rlin225/test/text_2000/\"\n",
    "summary_path = \"/home/rlin225/test/summary_2000/\"\n",
    "file_list = \"/home/rlin225/test/datalist.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embedding matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_counter = Counter()\n",
    "target_counter = Counter()\n",
    "max_input_seq_length = 0\n",
    "max_target_seq_length = 0\n",
    "input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n",
    "target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n",
    "\n",
    "data_df = pd.read_csv(file_list,sep=',',header='infer')\n",
    "\n",
    "for i in range(len(data_df)):\n",
    "    text_file = text_path+data_df[\"text_path\"][i]\n",
    "    with open(text_file, 'r') as rf:\n",
    "        text = rf.read()\n",
    "        text = [word.lower() for word in text.split(' ')]\n",
    "        seq_length = len(text)\n",
    "    if seq_length > input_seq_max_length:\n",
    "        text = text[0:input_seq_max_length]\n",
    "        seq_length = len(text)\n",
    "    for word in text:\n",
    "        input_counter[word] += 1\n",
    "    max_input_seq_length = max(max_input_seq_length, seq_length)\n",
    "    \n",
    "    summary_file = summary_path + data_df[\"summary_path\"][i]\n",
    "    with open(summary_file, 'r') as rf:\n",
    "        text = '\\t' + rf.read() + '\\n'\n",
    "        text = [word.lower() for word in text.split(' ')]\n",
    "        seq_length = len(text)\n",
    "    if seq_length > target_seq_max_length:\n",
    "        text = text[0:target_seq_max_length]\n",
    "        seq_length = len(text)\n",
    "    for word in text:\n",
    "        target_counter[word] += 1\n",
    "    max_target_seq_length = max(max_target_seq_length, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word2idx = dict()\n",
    "for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n",
    "    input_word2idx[word[0]] = idx + 2\n",
    "input_word2idx['PAD'] = 0\n",
    "input_word2idx['UNK'] = 1\n",
    "input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
    "\n",
    "target_word2idx = dict()\n",
    "for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n",
    "    target_word2idx[word[0]] = idx + 1\n",
    "target_word2idx['UNK'] = 0\n",
    "\n",
    "target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "\n",
    "num_input_tokens = len(input_word2idx)\n",
    "num_target_tokens = len(target_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_input_seq_length: 1802\n",
      "max_target_seq_length: 71\n",
      "num_input_tokens: 15002\n",
      "num_target_tokens: 6001\n"
     ]
    }
   ],
   "source": [
    "print('max_input_seq_length:', max_input_seq_length)\n",
    "print('max_target_seq_length:', max_target_seq_length)\n",
    "print('num_input_tokens:', num_input_tokens)\n",
    "print('num_target_tokens:', num_target_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "These are required to convert input texts to numerical representation, and also converting numbers back to text so that we can get summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input_text(texts):\n",
    "    \"\"\" Takes a list of news texts and batch process them.\"\"\"\n",
    "    temp = []\n",
    "    for input_news in texts:\n",
    "        x = []\n",
    "        for word in input_news.lower().split(' '):\n",
    "            wid = 1\n",
    "            if word in input_word2idx:\n",
    "                wid = input_word2idx[word]\n",
    "            x.append(wid)\n",
    "            if len(x) >= max_input_seq_length:\n",
    "                break\n",
    "        temp.append(x)\n",
    "    temp = pad_sequences(temp, maxlen=max_input_seq_length)\n",
    "    print(temp.shape)\n",
    "    return temp\n",
    "\n",
    "def transform_target_encoding(texts):\n",
    "    \"\"\" Processing target text sequences here. (Actual encoding translation happens at the generator)\"\"\"\n",
    "    temp = []\n",
    "    for line in texts:\n",
    "        x = []\n",
    "        line2 = '\\t' + line.lower() + '\\n'\n",
    "        for word in line2.split(' '):\n",
    "            x.append(word)\n",
    "            if len(x) >= max_target_seq_length:\n",
    "                break\n",
    "        temp.append(x)\n",
    "\n",
    "    temp = np.array(temp)\n",
    "    print(temp.shape)\n",
    "    return temp\n",
    "\n",
    "def generate_batch(x_samples, y_samples, batch_size):\n",
    "    \"\"\" Use generators here so as to reduce the burden at the training time \"\"\"\n",
    "    num_batches = len(x_samples) // batch_size\n",
    "    while True:\n",
    "        for batchIdx in range(0, num_batches):\n",
    "            start = batchIdx * batch_size\n",
    "            end = (batchIdx + 1) * batch_size\n",
    "            encoder_input_data_batch = pad_sequences(x_samples[start:end], max_input_seq_length)\n",
    "            decoder_target_data_batch = np.zeros(\n",
    "                shape=(batch_size, max_target_seq_length, num_target_tokens))\n",
    "            for lineIdx, target_words in enumerate(y_samples[start:end]):\n",
    "                for idx, w in enumerate(target_words):\n",
    "                    w2idx = 0  # default [UNK]\n",
    "                    if w in target_word2idx:\n",
    "                        w2idx = target_word2idx[w]\n",
    "                    if w2idx != 0:\n",
    "                        decoder_target_data_batch[lineIdx, idx, w2idx] = 1\n",
    "            yield encoder_input_data_batch, decoder_target_data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(input_text):\n",
    "    \"\"\" Given a news text, automatically generate the summary.\"\"\"\n",
    "    input_seq = []\n",
    "    input_wids = []\n",
    "    for word in input_text.lower().split(' '):\n",
    "        idx = 1  # default [UNK]\n",
    "        if word in input_word2idx:\n",
    "            idx = input_word2idx[word]\n",
    "        input_wids.append(idx)\n",
    "    input_seq.append(input_wids)\n",
    "    input_seq = pad_sequences(input_seq, max_input_seq_length)\n",
    "    predicted = self.model.predict(input_seq)\n",
    "    predicted_word_idx_list = np.argmax(predicted, axis=1)\n",
    "    predicted_word_list = [self.target_idx2word[wid] for wid in predicted_word_idx_list[0]]\n",
    "    return predicted_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "The meat is here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_LSTM():\n",
    "    \"\"\" This is a simple One-shot model, can add more\"\"\"\n",
    "    # encoder input model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(output_dim=128, input_dim=num_input_tokens, input_length=max_input_seq_length))\n",
    "\n",
    "    # encoder model\n",
    "    model.add(LSTM(128))\n",
    "    model.add(RepeatVector(max_target_seq_length))\n",
    "    # decoder model\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(num_target_tokens, activation='softmax')))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The code below is a demo for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "data_df = pd.read_csv(file_list,sep=',',header='infer')\n",
    "\n",
    "# get the data and store them into an array\n",
    "print(\"Loading the data...\")\n",
    "for i in range(len(data_df)):\n",
    "    text_file = text_path+data_df[\"text_path\"][i]\n",
    "    with open(text_file, 'r') as rf:\n",
    "        text = rf.read()\n",
    "        X.append(text)\n",
    "        \n",
    "    summary_file = summary_path + data_df[\"summary_path\"][i]\n",
    "    with open(summary_file, 'r') as rf:\n",
    "        text = rf.read()\n",
    "        y.append(text)\n",
    "\n",
    "# do a split\n",
    "print(\"Spliting the data...\")\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print('training size: ', len(Xtrain))\n",
    "print('testing size: ', len(Xtest))\n",
    "\n",
    "Ytrain = transform_target_encoding(Ytrain)\n",
    "Ytest = transform_target_encoding(Ytest)\n",
    "\n",
    "Xtrain = transform_input_text(Xtrain)\n",
    "Xtest = transform_input_text(Xtest)\n",
    "\n",
    "train_gen = generate_batch(Xtrain, Ytrain, batch_size)\n",
    "test_gen = generate_batch(Xtest, Ytest, batch_size)\n",
    "\n",
    "train_num_batches = len(Xtrain) // batch_size\n",
    "test_num_batches = len(Xtest) // batch_size\n",
    "\n",
    "# train a model\n",
    "print('Start fitting ...')\n",
    "model = CNN_LSTM()\n",
    "model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                   epochs=epochs,\n",
    "                   verbose=VERBOSE, validation_data=test_gen, validation_steps=test_num_batches,\n",
    "                   callbacks=[checkpoint])\n",
    "model.save('lstm_summary.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Below shows how to do inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('lstm_summary.h5')\n",
    "ran = random.randint(0,len(data_df)) - 1\n",
    "text_file = text_path+data_df[\"text_path\"][ran]\n",
    "with open(text_file, 'r') as rf:\n",
    "    test_text = rf.read()\n",
    "summary_file = summary_path + data_df[\"summary_path\"][ran]\n",
    "with open(summary_file, 'r') as rf:\n",
    "    ground_truth = rf.read()\n",
    "auto_summary = summarize(test_text)\n",
    "print(\"Auto summary:\",auto_summary)\n",
    "print(\"Actual summary:\",ground_truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
